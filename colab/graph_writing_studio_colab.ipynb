{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "name": "graph_writing_studio_colab.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Graph Writing Studio ‚Äî Google Colab\n",
    "\n",
    "Run the full **Graph Writing Studio** pipeline in the cloud ‚Äî no local GPU, Docker, or Neo4j installation required.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Installs Ollama and starts it as a background service\n",
    "2. Installs Neo4j Community Edition directly (no Docker needed)\n",
    "3. Clones the project and installs Python dependencies\n",
    "4. Walks through the complete entity and conversation workflows\n",
    "\n",
    "**Hardware requirements:**\n",
    "- Works on **CPU-only** Colab instances (free tier)\n",
    "- Faster with a **T4 or L4 GPU** (Colab free / Pro)\n",
    "- Neo4j requires ~512 MB RAM; a free Colab instance has 12‚Äì13 GB ‚Äî plenty\n",
    "\n",
    "**Runtime:** `Runtime ‚Üí Change runtime type ‚Üí Python 3` (GPU optional but recommended)\n",
    "\n",
    "---\n",
    "> **Note:** Each Colab session starts fresh. Run cells top-to-bottom on a new session.\n",
    "> To avoid re-downloading large models, see **Step 6: Persist Models to Google Drive**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Install Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-ollama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system utilities and Ollama\n",
    "# pciutils / lshw are used to detect whether a GPU is present\n",
    "!sudo apt-get update -qq && sudo apt-get install -y -q pciutils lshw\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "print(\"‚úÖ Ollama installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Start Ollama Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "start-ollama",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def start_ollama_server():\n",
    "    \"\"\"Kill any stale process, then start ollama serve in the background.\"\"\"\n",
    "    subprocess.run(\"pkill ollama\", shell=True, stderr=subprocess.DEVNULL)\n",
    "    time.sleep(1)\n",
    "    with open(\"/tmp/ollama.log\", \"w\") as log:\n",
    "        subprocess.Popen([\"ollama\", \"serve\"], stdout=log, stderr=log)\n",
    "    time.sleep(3)\n",
    "\n",
    "def wait_for_ollama(timeout=60):\n",
    "    \"\"\"Poll the Ollama API until it responds or the timeout expires.\"\"\"\n",
    "    for i in range(timeout):\n",
    "        try:\n",
    "            r = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "            if r.status_code == 200:\n",
    "                return True\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "    return False\n",
    "\n",
    "start_ollama_server()\n",
    "\n",
    "if wait_for_ollama():\n",
    "    print(\"‚úÖ Ollama server is running at http://localhost:11434\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama failed to start ‚Äî check /tmp/ollama.log\")\n",
    "    !tail -20 /tmp/ollama.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Choose and Pull a Model\n",
    "\n",
    "The cell below auto-detects whether a GPU is present and suggests the best model for your hardware.\n",
    "\n",
    "| Model | VRAM / RAM | Speed | Quality |\n",
    "|---|---|---|---|\n",
    "| `llama3.2:1b` | ~1 GB | ‚ö° Very fast (CPU OK) | Basic |\n",
    "| `llama3.2:3b` | ~2 GB | Fast (CPU OK) | **Good** |\n",
    "| `phi3:mini` | ~2.3 GB | Fast (CPU OK) | Good |\n",
    "| `llama3.1:8b` | ~5 GB | Moderate | Better |\n",
    "| `llama3.1:70b` | ~40 GB | Slow (needs GPU) | Best |\n",
    "\n",
    "You can override the auto-selected model in the form below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def detect_gpu():\n",
    "    \"\"\"Return True if a CUDA-capable GPU is visible to the system.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"],\n",
    "            capture_output=True, text=True, timeout=5\n",
    "        )\n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            return True, result.stdout.strip().splitlines()[0]\n",
    "    except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "        pass\n",
    "    return False, None\n",
    "\n",
    "has_gpu, gpu_name = detect_gpu()\n",
    "\n",
    "if has_gpu:\n",
    "    print(f\"üü¢ GPU detected: {gpu_name}\")\n",
    "    print(\"   Recommended model: llama3.1:8b  (upgrade to llama3.1:70b for best quality)\")\n",
    "    suggested_model = \"llama3.1:8b\"\n",
    "else:\n",
    "    print(\"üü° No GPU detected ‚Äî running on CPU\")\n",
    "    print(\"   Recommended model: llama3.2:3b  (good quality, ~2 GB download)\")\n",
    "    suggested_model = \"llama3.2:3b\"\n",
    "\n",
    "print(f\"\\nSuggested model: {suggested_model}\")\n",
    "print(\"Override this in the next cell if you prefer a different model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pull-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Model selection { run: \"auto\" }\n",
    "# Choose a model. llama3.2:3b works well on CPU. llama3.1:8b needs a GPU or patience.\n",
    "MODEL_NAME = \"llama3.2:3b\"  # @param [\"llama3.2:1b\", \"llama3.2:3b\", \"phi3:mini\", \"llama3.1:8b\", \"llama3.1:70b\"]\n",
    "\n",
    "print(f\"Pulling {MODEL_NAME} ‚Äî this may take several minutes on first run...\")\n",
    "!ollama pull {MODEL_NAME}\n",
    "print(f\"\\n‚úÖ {MODEL_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Install and Start Neo4j\n",
    "\n",
    "Graph Writing Studio stores all extracted entities and relationships in **Neo4j**.  \n",
    "We install Neo4j Community Edition 5.x directly into the Colab VM ‚Äî no Docker needed.\n",
    "\n",
    "> This installs the **Graph Data Science (GDS)** plugin, which is required for community detection (Louvain / Leiden algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-neo4j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Neo4j's apt repository and install Community Edition + GDS plugin\n",
    "!curl -fsSL https://debian.neo4j.com/neotechnology.gpg.key \\\n",
    "    | gpg --dearmor -o /usr/share/keyrings/neo4j.gpg\n",
    "!echo \"deb [signed-by=/usr/share/keyrings/neo4j.gpg] https://debian.neo4j.com stable 5\" \\\n",
    "    > /etc/apt/sources.list.d/neo4j.list\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -q neo4j\n",
    "print(\"‚úÖ Neo4j installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-neo4j",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "NEO4J_PASSWORD = \"graphstudio\"  # Must match the .env written later\n",
    "\n",
    "# ‚îÄ‚îÄ Set initial password (required before first start) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "result = subprocess.run(\n",
    "    [\"neo4j-admin\", \"dbms\", \"set-initial-password\", NEO4J_PASSWORD],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode != 0:\n",
    "    # Password may already be set if the cell is re-run\n",
    "    print(f\"Password note: {result.stderr.strip() or result.stdout.strip()}\")\n",
    "\n",
    "# ‚îÄ‚îÄ Download and install the GDS plugin ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# GDS jar must live in the Neo4j plugins directory\n",
    "GDS_VERSION = \"2.6.8\"  # Compatible with Neo4j 5.x Community\n",
    "GDS_JAR = f\"neo4j-graph-data-science-{GDS_VERSION}.jar\"\n",
    "GDS_URL = f\"https://graphdatascience.ninja/neo4j-graph-data-science-{GDS_VERSION}.jar\"\n",
    "PLUGINS_DIR = \"/var/lib/neo4j/plugins\"\n",
    "\n",
    "print(f\"Downloading GDS plugin v{GDS_VERSION}...\")\n",
    "!wget -q \"{GDS_URL}\" -O \"/tmp/{GDS_JAR}\"\n",
    "!cp \"/tmp/{GDS_JAR}\" \"{PLUGINS_DIR}/{GDS_JAR}\"\n",
    "!chown neo4j:neo4j \"{PLUGINS_DIR}/{GDS_JAR}\"\n",
    "print(\"‚úÖ GDS plugin installed.\")\n",
    "\n",
    "# ‚îÄ‚îÄ Allow GDS in neo4j.conf ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "conf_path = \"/etc/neo4j/neo4j.conf\"\n",
    "with open(conf_path, \"a\") as f:\n",
    "    f.write(\"\\n# Graph Data Science plugin\\n\")\n",
    "    f.write(\"dbms.security.procedures.unrestricted=gds.*\\n\")\n",
    "    f.write(\"dbms.security.procedures.allowlist=gds.*\\n\")\n",
    "print(\"‚úÖ neo4j.conf updated for GDS.\")\n",
    "\n",
    "# ‚îÄ‚îÄ Start Neo4j ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "subprocess.run([\"neo4j\", \"start\"], capture_output=True)\n",
    "print(\"Starting Neo4j...\")\n",
    "\n",
    "def wait_for_neo4j(timeout=90):\n",
    "    \"\"\"Poll the Neo4j HTTP endpoint until it is ready.\"\"\"\n",
    "    for i in range(timeout):\n",
    "        try:\n",
    "            r = requests.get(\"http://localhost:7474\", timeout=2)\n",
    "            if r.status_code in (200, 401):\n",
    "                return True\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "        if i % 10 == 9:\n",
    "            print(f\"  Still waiting... ({i+1}s)\")\n",
    "    return False\n",
    "\n",
    "if wait_for_neo4j():\n",
    "    print(\"‚úÖ Neo4j is running.\")\n",
    "    print(\"   Browser UI:  http://localhost:7474  (use the ngrok cell below to access it)\")\n",
    "    print(\"   Bolt:        bolt://localhost:7687\")\n",
    "else:\n",
    "    print(\"‚ùå Neo4j did not start in time ‚Äî check logs below\")\n",
    "    !neo4j status\n",
    "    !tail -30 /var/log/neo4j/neo4j.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Clone the Project and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/graph-writing-studio\"\n",
    "\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    !git clone https://github.com/dms-killa/graph-writing-studio \"{REPO_DIR}\"\n",
    "else:\n",
    "    print(\"Repo already cloned ‚Äî pulling latest changes.\")\n",
    "    !git -C \"{REPO_DIR}\" pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "print(\"‚úÖ Python dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-config-header",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Configure Environment\n",
    "\n",
    "The project reads settings from a `.env` file. We write it here so that `config.py` picks up the right Ollama model and Neo4j credentials automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# MODEL_NAME was set in Step 3. Re-assign here if you skipped that cell.\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"llama3.2:3b\")\n",
    "\n",
    "env_content = f\"\"\"# Graph Writing Studio ‚Äî Colab configuration\n",
    "# Generated automatically by graph_writing_studio_colab.ipynb\n",
    "\n",
    "# Ollama settings\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "OLLAMA_MODEL={MODEL_NAME}\n",
    "OLLAMA_TEMPERATURE=0.1\n",
    "OLLAMA_TIMEOUT=300.0\n",
    "\n",
    "# Neo4j settings\n",
    "NEO4J_URI=bolt://localhost:7687\n",
    "NEO4J_USER=neo4j\n",
    "NEO4J_PASSWORD=graphstudio\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "# Also export to the process environment so subprocesses inherit them\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\"\n",
    "os.environ[\"OLLAMA_MODEL\"]    = MODEL_NAME\n",
    "os.environ[\"NEO4J_URI\"]       = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USER\"]      = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"]  = \"graphstudio\"\n",
    "\n",
    "print(\"‚úÖ .env written.\")\n",
    "print(f\"   Ollama model : {MODEL_NAME}\")\n",
    "print( \"   Neo4j URI    : bolt://localhost:7687\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-verify-header",
   "metadata": {},
   "source": [
    "## Step 7 ‚Äî Verify Connections\n",
    "\n",
    "Confirm that both Ollama and Neo4j are reachable before running any workflow steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-connections",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ‚îÄ‚îÄ Ollama ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    r = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    models = [m[\"name\"] for m in r.json().get(\"models\", [])]\n",
    "    print(f\"‚úÖ Ollama ‚Äî available models: {models}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama connection failed: {e}\")\n",
    "\n",
    "# ‚îÄ‚îÄ Neo4j ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    driver = GraphDatabase.driver(\n",
    "        \"bolt://localhost:7687\",\n",
    "        auth=(\"neo4j\", \"graphstudio\")\n",
    "    )\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"RETURN 'connected' AS status\")\n",
    "        record = result.single()\n",
    "        print(f\"‚úÖ Neo4j  ‚Äî {record['status']}\")\n",
    "    driver.close()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Neo4j connection failed: {e}\")\n",
    "    print(\"   If this is the first run, wait 30 seconds and retry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-workflow-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 ‚Äî Entity Workflow\n",
    "\n",
    "Run the full pipeline on the bundled contact sample files.\n",
    "\n",
    "### 8a ‚Äî Ingest Contact Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ingest-contacts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry-run first: extract entities without writing to Neo4j\n",
    "print(\"=== Dry-run extraction (no Neo4j write) ===\")\n",
    "!python main.py ingest --source samples/john_smith.txt --dry-run\n",
    "\n",
    "print(\"\\n=== Full ingestion ===\")\n",
    "!python main.py ingest --source samples/john_smith.txt\n",
    "!python main.py ingest --source samples/sarah_chen.txt\n",
    "print(\"\\n‚úÖ Both contacts ingested.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8b-header",
   "metadata": {},
   "source": [
    "### 8b ‚Äî Discover the Outline via Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Outline algorithm { run: \"auto\" }\n",
    "OUTLINE_ALGORITHM = \"louvain\"  # @param [\"louvain\", \"leiden\"]\n",
    "\n",
    "!python main.py outline --algorithm {OUTLINE_ALGORITHM}\n",
    "\n",
    "import json, pathlib\n",
    "outline_path = pathlib.Path(\"outline.json\")\n",
    "if outline_path.exists():\n",
    "    outline = json.loads(outline_path.read_text())\n",
    "    print(f\"\\n‚úÖ Outline saved ‚Äî {len(outline)} section(s) detected.\")\n",
    "    for i, section in enumerate(outline):\n",
    "        members = section.get(\"members\", [])\n",
    "        print(f\"  Section {i}: {members}\")\n",
    "else:\n",
    "    print(\"outline.json not found ‚Äî check the output above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8c-header",
   "metadata": {},
   "source": [
    "### 8c ‚Äî Generate a Drafting Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Draft section { run: \"auto\" }\n",
    "SECTION_INDEX = 0  # @param {type:\"integer\"}\n",
    "\n",
    "!python main.py draft --section {SECTION_INDEX}\n",
    "\n",
    "import pathlib\n",
    "prompt_path = pathlib.Path(f\"drafts/section_{SECTION_INDEX}_prompt.txt\")\n",
    "if prompt_path.exists():\n",
    "    print(f\"\\n‚úÖ Prompt saved to {prompt_path}\")\n",
    "    print(\"\\n--- Prompt preview (first 40 lines) ---\")\n",
    "    lines = prompt_path.read_text().splitlines()\n",
    "    print(\"\\n\".join(lines[:40]))\n",
    "    if len(lines) > 40:\n",
    "        print(f\"\\n... ({len(lines) - 40} more lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8d-header",
   "metadata": {},
   "source": [
    "### 8d ‚Äî Send the Prompt to Ollama (Optional)\n",
    "\n",
    "Pipe the generated prompt directly to the local model for an end-to-end draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ollama-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate draft with Ollama { run: \"auto\" }\n",
    "# NOTE: On CPU this may take several minutes. Monitor /tmp/ollama.log if it hangs.\n",
    "SECTION_TO_DRAFT = 0  # @param {type:\"integer\"}\n",
    "\n",
    "import subprocess, pathlib\n",
    "\n",
    "prompt_file = pathlib.Path(f\"drafts/section_{SECTION_TO_DRAFT}_prompt.txt\")\n",
    "if not prompt_file.exists():\n",
    "    print(f\"Run '8c ‚Äî Generate a Drafting Prompt' for section {SECTION_TO_DRAFT} first.\")\n",
    "else:\n",
    "    print(f\"Sending {prompt_file} to {MODEL_NAME}...\")\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", MODEL_NAME],\n",
    "        input=prompt_file.read_text(),\n",
    "        capture_output=True, text=True, timeout=600\n",
    "    )\n",
    "    draft_output = result.stdout.strip()\n",
    "    print(\"\\n=== Draft Output ===\")\n",
    "    print(draft_output)\n",
    "\n",
    "    output_path = pathlib.Path(f\"drafts/section_{SECTION_TO_DRAFT}_draft.txt\")\n",
    "    output_path.write_text(draft_output)\n",
    "    print(f\"\\n‚úÖ Draft saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8e-header",
   "metadata": {},
   "source": [
    "### 8e ‚Äî Store Editorial Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "store-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Feedback form\n",
    "FEEDBACK_TYPE   = \"AVOID_TOPIC\"        # @param [\"AVOID_TOPIC\", \"PREFER_STYLE\", \"CORRECT_FACT\", \"MERGE_ENTITIES\", \"CONFIRM_RELATION\", \"REJECT_RELATION\"]\n",
    "TARGET_ENTITY   = \"John Smith\"         # @param {type:\"string\"}\n",
    "INSTRUCTION     = \"Don't mention salary or compensation details\"  # @param {type:\"string\"}\n",
    "\n",
    "!python main.py feedback \\\n",
    "    --type        \"{FEEDBACK_TYPE}\" \\\n",
    "    --entity      \"{TARGET_ENTITY}\" \\\n",
    "    --instruction \"{INSTRUCTION}\"\n",
    "print(\"\\n‚úÖ Feedback stored as a graph node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9 ‚Äî Conversation Workflow\n",
    "\n",
    "Graph Writing Studio can also ingest conversation transcripts, detect rhetorical tactics, and cluster messages into sections using community detection.\n",
    "\n",
    "### 9a ‚Äî Ingest Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ingest-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERSATION_SOURCE = \"samples/democratic_backsliding_chat.md\"\n",
    "CONVERSATION_ID     = \"democratic_backsliding_chat\"\n",
    "\n",
    "!python main.py ingest \\\n",
    "    --source       \"{CONVERSATION_SOURCE}\" \\\n",
    "    --conversation\n",
    "print(\"\\n‚úÖ Conversation ingested.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9b-header",
   "metadata": {},
   "source": [
    "### 9b ‚Äî Conversation Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conversation-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Conversation outline algorithm { run: \"auto\" }\n",
    "CONV_ALGORITHM = \"leiden\"  # @param [\"leiden\", \"louvain\"]\n",
    "\n",
    "!python main.py outline-conversation \"{CONVERSATION_ID}\" --algorithm {CONV_ALGORITHM}\n",
    "\n",
    "import json, pathlib\n",
    "conv_outline_path = pathlib.Path(\"outline_conversation.json\")\n",
    "if conv_outline_path.exists():\n",
    "    conv_outline = json.loads(conv_outline_path.read_text())\n",
    "    print(f\"\\n‚úÖ Conversation outline: {len(conv_outline)} section(s)\")\n",
    "    for i, section in enumerate(conv_outline):\n",
    "        msg_ids = section.get(\"message_ids\", [])\n",
    "        print(f\"  Section {i}: {len(msg_ids)} message(s) ‚Äî {msg_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9c-header",
   "metadata": {},
   "source": [
    "### 9c ‚Äî Draft Conversation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "draft-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Draft conversation section { run: \"auto\" }\n",
    "CONV_SECTION = 0  # @param {type:\"integer\"}\n",
    "\n",
    "!python main.py draft-conversation \\\n",
    "    --section       {CONV_SECTION} \\\n",
    "    --conversation  \"{CONVERSATION_ID}\"\n",
    "\n",
    "import pathlib\n",
    "conv_prompt_path = pathlib.Path(f\"drafts/conversation_section_{CONV_SECTION}_prompt.txt\")\n",
    "if conv_prompt_path.exists():\n",
    "    print(f\"\\n‚úÖ Prompt saved to {conv_prompt_path}\")\n",
    "    lines = conv_prompt_path.read_text().splitlines()\n",
    "    print(\"\\n--- Prompt preview ---\")\n",
    "    print(\"\\n\".join(lines[:40]))\n",
    "    if len(lines) > 40:\n",
    "        print(f\"\\n... ({len(lines) - 40} more lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step10-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10 ‚Äî Ingest Your Own Files\n",
    "\n",
    "Upload your own text files or conversation transcripts and run the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-file-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Upload and ingest a custom file\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "uploaded = files.upload()  # Opens a file picker dialog\n",
    "\n",
    "for filename, data in uploaded.items():\n",
    "    dest = os.path.join(\"samples\", filename)\n",
    "    with open(dest, \"wb\") as f:\n",
    "        f.write(data)\n",
    "    print(f\"Saved to {dest}\")\n",
    "\n",
    "print(\"\\nRun the cells in Step 8 or 9 with the path above to ingest your file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ingest-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Ingest custom file { run: \"auto\" }\n",
    "CUSTOM_SOURCE      = \"samples/your_file.txt\"  # @param {type:\"string\"}\n",
    "IS_CONVERSATION    = False                     # @param {type:\"boolean\"}\n",
    "MIN_CONFIDENCE     = 0.5                       # @param {type:\"number\"}\n",
    "\n",
    "import shlex\n",
    "conv_flag = \"--conversation\" if IS_CONVERSATION else \"\"\n",
    "\n",
    "!python main.py ingest \\\n",
    "    --source           \"{CUSTOM_SOURCE}\" \\\n",
    "    --min-confidence   {MIN_CONFIDENCE} \\\n",
    "    {conv_flag}\n",
    "print(\"\\n‚úÖ Done. Now run outline ‚Üí draft.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step11-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11 ‚Äî (Optional) Expose Neo4j Browser via Ngrok\n",
    "\n",
    "If you want to inspect the graph visually using the Neo4j Browser, you can expose port 7474 with **ngrok** (free plan, no credit card required).\n",
    "\n",
    "1. Sign up at https://ngrok.com and copy your auth token\n",
    "2. Paste it in the form below and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ngrok-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Expose Neo4j Browser\n",
    "NGROK_AUTH_TOKEN = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "if not NGROK_AUTH_TOKEN:\n",
    "    print(\"Paste your ngrok auth token above and re-run this cell.\")\n",
    "else:\n",
    "    !pip install -q pyngrok\n",
    "    from pyngrok import ngrok, conf\n",
    "    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
    "    tunnel = ngrok.connect(7474, \"http\")\n",
    "    print(f\"‚úÖ Neo4j Browser: {tunnel.public_url}\")\n",
    "    print(f\"   Login: neo4j / graphstudio\")\n",
    "    print(f\"   Bolt connection string: bolt://localhost:7687\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step12-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12 ‚Äî (Optional) Persist Ollama Models to Google Drive\n",
    "\n",
    "Ollama models are 2‚Äì40 GB. Re-downloading them on every session is slow.  \n",
    "This cell mounts your Google Drive and symlinks the Ollama model cache so downloads persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persist-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "DRIVE_MODELS_DIR = \"/content/drive/MyDrive/ollama_models\"\n",
    "OLLAMA_MODELS    = \"/root/.ollama/models\"\n",
    "\n",
    "# Stop Ollama before moving the model directory\n",
    "subprocess.run(\"pkill ollama\", shell=True, stderr=subprocess.DEVNULL)\n",
    "time.sleep(2)\n",
    "\n",
    "import os, shutil\n",
    "os.makedirs(DRIVE_MODELS_DIR, exist_ok=True)\n",
    "\n",
    "if os.path.isdir(OLLAMA_MODELS) and not os.path.islink(OLLAMA_MODELS):\n",
    "    # First time: move local models to Drive, then symlink back\n",
    "    shutil.move(OLLAMA_MODELS, DRIVE_MODELS_DIR)\n",
    "    print(f\"Moved models to {DRIVE_MODELS_DIR}\")\n",
    "elif os.path.islink(OLLAMA_MODELS):\n",
    "    os.unlink(OLLAMA_MODELS)\n",
    "    print(\"Removed old symlink.\")\n",
    "\n",
    "os.symlink(DRIVE_MODELS_DIR, OLLAMA_MODELS)\n",
    "print(f\"‚úÖ Symlinked {OLLAMA_MODELS} ‚Üí {DRIVE_MODELS_DIR}\")\n",
    "\n",
    "# Restart Ollama with the Drive-backed model cache\n",
    "start_ollama_server()  # defined in Step 2\n",
    "if wait_for_ollama():\n",
    "    print(\"‚úÖ Ollama restarted with persistent model cache.\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama failed to restart ‚Äî re-run the Step 2 cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting\n",
    "\n",
    "| Symptom | Fix |\n",
    "|---|---|\n",
    "| `Ollama server failed to start` | `!tail -30 /tmp/ollama.log` ‚Äî often a port conflict. Re-run Step 2. |\n",
    "| `Neo4j connection failed` | Wait 30 s and retry. Check `!neo4j status` and `!tail -30 /var/log/neo4j/neo4j.log`. |\n",
    "| `GDS plugin not found` | Confirm the jar is in `/var/lib/neo4j/plugins/` and `neo4j.conf` has the allowlist lines. |\n",
    "| Extraction is very slow | Switch to `llama3.2:1b` in Step 3 for faster (less accurate) extraction on CPU. |\n",
    "| `ollama run` hangs | Increase `OLLAMA_TIMEOUT` in `.env` (default 300 s). On CPU, large prompts can exceed this. |\n",
    "| Colab disconnects | Re-run all setup cells (Steps 1‚Äì7) ‚Äî the VM state is lost on disconnect. Models on Drive (Step 12) survive. |\n",
    "| `ModuleNotFoundError` | Make sure `%cd /content/graph-writing-studio` ran. Re-run Step 5. |\n",
    "\n",
    "### Useful log commands\n",
    "\n",
    "```python\n",
    "!tail -30 /tmp/ollama.log           # Ollama server log\n",
    "!tail -30 /var/log/neo4j/neo4j.log  # Neo4j server log\n",
    "!neo4j status                       # Neo4j process status\n",
    "!ollama list                        # Confirm pulled models\n",
    "```"
   ]
  }
 ]
}
